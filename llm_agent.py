import os
import json
from typing import Dict, Any, Optional
from dotenv import load_dotenv
import openai
from tool_manager import ToolManager
from logger import get_logger

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class LLMStockAgent:
    def __init__(self, news_api_key: str, model_name: str = "gpt-4"):
        self.model_name = model_name
        self.tool_manager = ToolManager(news_api_key)
        self.openai_client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

        # ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰Agentçš„è§’è‰²å’Œè¡Œä¸ºå‡†åˆ™
        self.system_prompt = self._create_system_prompt()

        # å¯¹è¯å†å²
        self.conversation_history = [{"role": "system", "content": self.system_prompt}]

    def _create_system_prompt(self) -> str:
        """åˆ›å»ºç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰Agentçš„è¡Œä¸ºæ–¹å¼"""
        tool_descriptions = self.tool_manager.get_all_tool_descriptions()

        # è·å–å½“å‰ç³»ç»Ÿæ—¥æœŸ
        from datetime import datetime

        current_date = datetime.now().strftime("%Y-%m-%d")

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æAIåŠ©æ‰‹ï¼Œä¸“æ³¨äºåŸºäºçœŸå®æ•°æ®çš„å®¢è§‚åˆ†æã€‚

ä»Šå¤©çš„æ—¥æœŸæ˜¯: {current_date}

**æ ¸å¿ƒåŸåˆ™ï¼š**
1. åªåŸºäºå·¥å…·è¿”å›çš„çœŸå®æ•°æ®è¿›è¡Œåˆ†æï¼Œç»ä¸ç¼–é€ æ•°æ®
2. æ˜ç¡®åŒºåˆ†äº‹å®å’Œæ¨æµ‹ï¼Œé¿å…è¿‡åº¦è§£è¯»
3. æ‰¿è®¤æ•°æ®å±€é™æ€§ï¼Œä¸åšç»å¯¹é¢„æµ‹
4. æä¾›é£é™©æç¤ºå’Œå…è´£å£°æ˜

**å¯ç”¨å·¥å…·ï¼š**
{tool_descriptions}

**åˆ†ææ¡†æ¶ï¼š**
1. **æ•°æ®æ”¶é›†é˜¶æ®µ**ï¼š
   - æ˜ç¡®ç”¨æˆ·éœ€æ±‚ï¼Œç¡®å®šæ‰€éœ€æ•°æ®ç±»å‹
   - ç³»ç»Ÿæ€§æ”¶é›†ç›¸å…³æ•°æ®ï¼ˆä»·æ ¼ã€è´¢åŠ¡ã€æŠ€æœ¯æŒ‡æ ‡ã€æ–°é—»ï¼‰
   - éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œæ—¶æ•ˆæ€§

2. **å®¢è§‚åˆ†æé˜¶æ®µ**ï¼š
   - æŠ€æœ¯é¢ï¼šåŸºäºæŒ‡æ ‡æ•°å€¼è¿›è¡Œè¶‹åŠ¿åˆ¤æ–­
   - åŸºæœ¬é¢ï¼šåŸºäºè´¢åŠ¡æ•°æ®è¯„ä¼°å…¬å¸å¥åº·åº¦
   - å¸‚åœºæƒ…ç»ªï¼šåŸºäºæ–°é—»å†…å®¹åˆ†æå¸‚åœºé¢„æœŸ
   - é£é™©è¯„ä¼°ï¼šè¯†åˆ«æ½œåœ¨é£é™©å› ç´ 

3. **ç»“è®ºè¡¨è¿°**ï¼š
   - æ˜ç¡®æ ‡æ³¨æ•°æ®æ¥æºå’Œæ—¶é—´
   - åŒºåˆ†"æ•°æ®æ˜¾ç¤º"å’Œ"å¯èƒ½æ„å‘³ç€"
   - æä¾›å¤šç§æƒ…æ™¯åˆ†æ
   - å¼ºè°ƒæŠ•èµ„é£é™©å’Œä¸ç¡®å®šæ€§

**å·¥å…·è°ƒç”¨æ ¼å¼ï¼š**
<tool_call>
{{
  "name": "å·¥å…·åç§°",
  "parameters": {{
    "å‚æ•°1": "å€¼1",
    "å‚æ•°2": "å€¼2"
  }}
}}
</tool_call>

**ä¸¥æ ¼è¦æ±‚ï¼š**
- ç¦æ­¢ç¼–é€ ä»»ä½•æ•°æ®æˆ–æŒ‡æ ‡å€¼
- å¦‚æœå·¥å…·è¿”å›é”™è¯¯æˆ–ç©ºæ•°æ®ï¼Œå¿…é¡»å¦‚å®è¯´æ˜
- ä¸å¾—å¯¹è‚¡ä»·åšå‡ºå…·ä½“çš„æ¶¨è·Œé¢„æµ‹
- å¿…é¡»åœ¨åˆ†æç»“å°¾åŒ…å«é£é™©æç¤º
- æ‰¿è®¤åˆ†æçš„å±€é™æ€§å’Œæ—¶æ•ˆæ€§

**æ ‡å‡†ç»“å°¾æ¨¡æ¿ï¼š**
"ä»¥ä¸Šåˆ†æåŸºäº{{æ•°æ®æ—¶é—´}}çš„å…¬å¼€æ•°æ®ï¼Œä»…ä¾›å‚è€ƒã€‚è‚¡å¸‚æŠ•èµ„å­˜åœ¨é£é™©ï¼Œè¿‡å¾€è¡¨ç°ä¸ä»£è¡¨æœªæ¥ç»“æœã€‚æŠ•èµ„è€…åº”ç»“åˆè‡ªèº«æƒ…å†µè°¨æ…å†³ç­–ï¼Œå¿…è¦æ—¶å’¨è¯¢ä¸“ä¸šæŠ•èµ„é¡¾é—®ã€‚"
"""

    def _parse_tool_call(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æå¤§æ¨¡å‹çš„å“åº”ï¼Œæå–å·¥å…·è°ƒç”¨æŒ‡ä»¤"""
        start_tag = "<tool_call>"
        end_tag = "</tool_call>"

        start_idx = response.find(start_tag)
        end_idx = response.find(end_tag)

        if start_idx == -1 or end_idx == -1:
            logger.debug("æœªåœ¨LLMå“åº”ä¸­æ‰¾åˆ°å·¥å…·è°ƒç”¨æ ‡è®°")
            return None

        try:
            tool_call_str = response[start_idx + len(start_tag) : end_idx].strip()
            logger.debug(f"æå–åˆ°å·¥å…·è°ƒç”¨æ–‡æœ¬: {tool_call_str[:100]}...")
            result = json.loads(tool_call_str)
            logger.debug(
                f"æˆåŠŸè§£æå·¥å…·è°ƒç”¨JSON: {result['name'] if 'name' in result else 'æœªçŸ¥å·¥å…·'}"
            )
            return result
        except json.JSONDecodeError:
            logger.error("è§£æå·¥å…·è°ƒç”¨JSONå¤±è´¥")
            return None

    def _run_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶éªŒè¯æ•°æ®è´¨é‡"""
        tool_name = tool_call.get("name")
        parameters = tool_call.get("parameters", {})

        logger.info(f"å‡†å¤‡æ‰§è¡Œå·¥å…·: {tool_name}")
        logger.debug(f"å·¥å…·å‚æ•°: {json.dumps(parameters)}")

        if not tool_name:
            logger.warning("æœªæŒ‡å®šå·¥å…·åç§°")
            return {"error": "æœªæŒ‡å®šå·¥å…·åç§°"}

        tool = self.tool_manager.get_tool(tool_name)
        if not tool:
            logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
            return {"error": f"æœªçŸ¥å·¥å…·: {tool_name}"}

        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œå·¥å…·: {tool_name}")
            result = tool.run(**parameters)

            # æ•°æ®éªŒè¯å’Œè´¨é‡æ£€æŸ¥
            validated_result = self._validate_tool_result(tool_name, result, parameters)
            return validated_result
        except Exception as e:
            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "tool": tool_name,
                "parameters": parameters,
                "error": str(e),
                "data_quality": "error",
            }

    def _validate_tool_result(
        self, tool_name: str, result: Any, parameters: Dict
    ) -> Dict[str, Any]:
        """éªŒè¯å·¥å…·è¿”å›ç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§"""
        validation_info = {
            "status": "success",
            "tool": tool_name,
            "parameters": parameters,
            "result": result,
            "data_quality": "good",
            "validation_notes": [],
        }

        return validation_info

    def analyze(
        self, user_query: str, max_steps: int = 5, step_callback=None
    ) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿›è¡Œåˆ†æ"""
        logger.info(f"å¼€å§‹åˆ†æç”¨æˆ·æŸ¥è¯¢: {user_query}")
        self.conversation_history.append({"role": "user", "content": user_query})

        steps = []
        final_analysis = None
        total_tokens_used = 0

        for step in range(max_steps):
            logger.info(f"æ‰§è¡Œåˆ†ææ­¥éª¤ {step+1}/{max_steps}")
            # è°ƒç”¨å¤§æ¨¡å‹è·å–å“åº”ï¼ˆä½¿ç”¨æµå¼APIï¼‰
            logger.debug(f"å‘OpenAIå‘é€æµå¼è¯·æ±‚ï¼Œæ¨¡å‹: {self.model_name}")
            
            # ä½¿ç”¨æµå¼APIè°ƒç”¨
            stream = self.openai_client.chat.completions.create(
                model=self.model_name, 
                messages=self.conversation_history,
                stream=True,
                stream_options={"include_usage": True}
            )

            # æ”¶é›†æµå¼å“åº”
            llm_response = ""
            step_tokens = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            is_tool_call = False
            for chunk in stream:
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿chunkæœ‰choicesä¸”ä¸ä¸ºç©º
                if hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
                    if chunk.choices[0].delta.content is not None:
                        content_chunk = chunk.choices[0].delta.content
                        llm_response += content_chunk
                        if "<tool_call>" in content_chunk.strip():
                            logger.debug(f"å‘ç°å·¥å…·è°ƒç”¨æ ‡è®°: {content_chunk}")
                            is_tool_call = True
                        # å®æ—¶å‘é€æµå¼å†…å®¹
                        if not is_tool_call and step_callback and content_chunk.strip():
                            step_callback({
                                "type": "stream",
                                "content": content_chunk,
                                "step": step + 1
                            })
                
                # è·å–tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆåªåœ¨æœ‰usageä¿¡æ¯çš„chunkä¸­æ›´æ–°ï¼‰
                if hasattr(chunk, 'usage') and chunk.usage:
                    step_tokens = {
                        "prompt_tokens": chunk.usage.prompt_tokens,
                        "completion_tokens": chunk.usage.completion_tokens,
                        "total_tokens": chunk.usage.total_tokens,
                    }
                    logger.debug(f"è·å–åˆ°tokenç»Ÿè®¡: {step_tokens}")
            
            total_tokens_used += step_tokens["total_tokens"]
            
            logger.info(
                f"OpenAIæµå¼APIè°ƒç”¨å®Œæˆ - æ­¥éª¤{step+1} Tokenä½¿ç”¨: {step_tokens['prompt_tokens']} prompt + {step_tokens['completion_tokens']} completion = {step_tokens['total_tokens']} total"
            )
            logger.debug(f"æ”¶åˆ°å®Œæ•´OpenAIå“åº”: {llm_response}")
            self.conversation_history.append(
                {"role": "assistant", "content": llm_response}
            )

            # è®°å½•æ­¥éª¤
            step_data = {
                "step": step + 1,
                "llm_response": llm_response,
                "token_usage": step_tokens,
            }
            steps.append(step_data)

            # å‘é€æ­¥éª¤å®Œæˆé€šçŸ¥
            if step_callback:
                step_callback({
                    "type": "step_complete",
                    "content": f"âœ… æ­¥éª¤ {step+1} å®Œæˆ",
                    "step": step + 1
                })

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            tool_call = self._parse_tool_call(llm_response)

            if not tool_call:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜åˆ†æå®Œæˆ
                logger.info("æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œåˆ†æå®Œæˆ")
                final_analysis = llm_response
                break

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            logger.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_call['name'] if tool_call else 'None'}")

            # å®æ—¶å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
            if step_callback and tool_call:
                tool_name = tool_call.get("name", "æœªçŸ¥å·¥å…·")
                tool_params = tool_call.get("parameters", {})

                # æ ¹æ®å·¥å…·ç±»å‹ç”Ÿæˆæè¿°
                if tool_name == "get_historical_data":
                    ticker = tool_params.get("ticker", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"ğŸ“Š æ­£åœ¨è·å– {ticker} çš„å†å²æ•°æ®...",
                        }
                    )
                elif tool_name == "get_financial_statements":
                    ticker = tool_params.get("ticker", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"ğŸ“ˆ æ­£åœ¨è·å– {ticker} çš„è´¢åŠ¡æŠ¥è¡¨...",
                        }
                    )
                elif tool_name == "get_news":
                    query = tool_params.get("query", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"ğŸ“° æ­£åœ¨è·å–å…³äº {query} çš„æœ€æ–°æ–°é—»...",
                        }
                    )
                elif tool_name == "calculate_technical_indicators":
                    ticker = tool_params.get("ticker", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"ğŸ“‰ æ­£åœ¨è®¡ç®— {ticker} çš„æŠ€æœ¯æŒ‡æ ‡...",
                        }
                    )
                elif tool_name == "get_stock_info":
                    ticker = tool_params.get("ticker", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"â„¹ï¸ æ­£åœ¨è·å– {ticker} çš„åŸºæœ¬ä¿¡æ¯...",
                        }
                    )
                elif tool_name == "search_web_info":
                    query = tool_params.get("query", "")
                    step_callback(
                        {
                            "type": "tool",
                            "content": f"ğŸ” æ­£åœ¨æœç´¢ç½‘ç»œä¿¡æ¯: {query}...",
                        }
                    )
                else:
                    step_callback(
                        {"type": "tool", "content": f"ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name}"}
                    )

            tool_result = self._run_tool(tool_call)
            steps[-1]["tool_call"] = tool_call
            steps[-1]["tool_result"] = tool_result

            # è®°å½•å·¥å…·è°ƒç”¨çš„è¯¦ç»†æ—¥å¿—
            tool_log_data = {
                "tool_name": tool_call.get("name", "unknown"),
                "tool_parameters": tool_call.get("parameters", {}),
                "tool_execution_status": tool_result.get("status", "unknown"),
                "data_quality": tool_result.get("data_quality", "unknown"),
                "validation_notes": tool_result.get("validation_notes", []),
                "tool_result": tool_result.get("result", {}),
            }
            logger.info(
                f"å·¥å…·è°ƒç”¨è¯¦æƒ…: {json.dumps(tool_log_data, ensure_ascii=False, indent=2)}"
            )

            # å‘é€å·¥å…·æ‰§è¡Œå®Œæˆä¿¡æ¯
            if step_callback:
                step_callback(
                    {"type": "thinking", "content": "âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨åˆ†æç»“æœ..."}
                )

            # å°†å·¥å…·ç»“æœè¿”å›ç»™å¤§æ¨¡å‹
            # å¤„ç†å¯èƒ½åŒ…å«Timestampç±»å‹é”®çš„å­—å…¸
            def json_serializable(obj):
                import pandas as pd

                if isinstance(obj, dict):
                    return {str(k): json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [json_serializable(i) for i in obj]
                elif isinstance(obj, pd.Timestamp):
                    return str(obj)
                else:
                    return obj

            serializable_result = json_serializable(tool_result)
            tool_result_msg = (
                f"å·¥å…·è°ƒç”¨ç»“æœ:\n{json.dumps(serializable_result, indent=2)}"
            )
            logger.debug(f"å·¥å…·è°ƒç”¨ç»“æœ: {json.dumps(serializable_result)}")
            self.conversation_history.append(
                {"role": "user", "content": tool_result_msg}
            )

        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ä½†è¿˜æ²¡æœ‰æœ€ç»ˆåˆ†æï¼Œè¯·æ±‚ä¸€ä¸ªæ€»ç»“
        if final_analysis is None:
            logger.info("è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œè¯·æ±‚æœ€ç»ˆåˆ†ææ€»ç»“")
            summary_prompt = "è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªå®Œæ•´çš„åˆ†ææ€»ç»“å’ŒæŠ•èµ„å»ºè®®ã€‚ä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ã€‚"
            self.conversation_history.append(
                {"role": "user", "content": summary_prompt}
            )

            # ä½¿ç”¨æµå¼APIè¿›è¡Œæœ€ç»ˆæ€»ç»“
            stream = self.openai_client.chat.completions.create(
                model=self.model_name, 
                messages=self.conversation_history,
                stream=True,
                stream_options={"include_usage": True}
            )

            # æ”¶é›†æµå¼å“åº”
            final_analysis = ""
            final_tokens = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            
            # å‘é€æœ€ç»ˆæ€»ç»“å¼€å§‹é€šçŸ¥
            if step_callback:
                step_callback({
                    "type": "final_start",
                    "content": "ğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆåˆ†ææ€»ç»“..."
                })
            
            for chunk in stream:
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿chunkæœ‰choicesä¸”ä¸ä¸ºç©º
                if hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
                    if chunk.choices[0].delta.content is not None:
                        content_chunk = chunk.choices[0].delta.content
                        final_analysis += content_chunk
                        
                        # å®æ—¶å‘é€æœ€ç»ˆåˆ†æçš„æµå¼å†…å®¹
                        if step_callback and content_chunk.strip():
                            step_callback({
                                "type": "final_stream",
                                "content": content_chunk
                            })
                
                # è·å–tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆåªåœ¨æœ‰usageä¿¡æ¯çš„chunkä¸­æ›´æ–°ï¼‰
                if hasattr(chunk, 'usage') and chunk.usage:
                    final_tokens = {
                        "prompt_tokens": chunk.usage.prompt_tokens,
                        "completion_tokens": chunk.usage.completion_tokens,
                        "total_tokens": chunk.usage.total_tokens,
                    }
                    logger.debug(f"æœ€ç»ˆæ€»ç»“è·å–åˆ°tokenç»Ÿè®¡: {final_tokens}")
            
            total_tokens_used += final_tokens["total_tokens"]
            
            logger.info(
                f"æœ€ç»ˆåˆ†ææ€»ç»“å®Œæˆ - Tokenä½¿ç”¨: {final_tokens['prompt_tokens']} prompt + {final_tokens['completion_tokens']} completion = {final_tokens['total_tokens']} total"
            )

            # è®°å½•æœ€ç»ˆæ€»ç»“æ­¥éª¤
            steps.append(
                {
                    "step": len(steps) + 1,
                    "llm_response": final_analysis,
                    "token_usage": final_tokens,
                    "is_final_summary": True,
                }
            )

        # å‘é€æœ€ç»ˆåˆ†æç»“æœ
        if step_callback and final_analysis:
            step_callback({"type": "final", "content": final_analysis})

        # è®°å½•æ€»ä½“tokenä½¿ç”¨ç»Ÿè®¡
        logger.info(
            f"åˆ†æå®Œæˆ - æ€»Tokenæ¶ˆè€—: {total_tokens_used} tokensï¼Œå…±æ‰§è¡Œ{len(steps)}ä¸ªæ­¥éª¤"
        )

        return {
            "query": user_query,
            "steps": steps,
            "final_analysis": final_analysis,
            "completed": final_analysis is not None,
            "total_tokens_used": total_tokens_used,
            "steps_count": len(steps),
        }
