import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import openai
from tools import ToolManager, Tool
from logger import get_logger

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMStockAgent:
    def __init__(self, news_api_key: str, model_name: str = "gpt-4"):
        self.model_name = model_name
        self.tool_manager = ToolManager(news_api_key)
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",)
        
        # ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰Agentçš„è§’è‰²å’Œè¡Œä¸ºå‡†åˆ™
        self.system_prompt = self._create_system_prompt()
        
        # å¯¹è¯å†å²
        self.conversation_history = [{"role": "system", "content": self.system_prompt}]
    
    def _create_system_prompt(self) -> str:
        """åˆ›å»ºç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰Agentçš„è¡Œä¸ºæ–¹å¼"""
        tool_descriptions = self.tool_manager.get_all_tool_descriptions()
        
        # è·å–å½“å‰ç³»ç»Ÿæ—¥æœŸ
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æAIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ†æè‚¡ç¥¨æ•°æ®å¹¶æä¾›æŠ•èµ„è§è§£ã€‚

ä»Šå¤©çš„æ—¥æœŸæ˜¯: {current_date}

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥è·å–æ‰€éœ€çš„æ•°æ®ï¼š

{tool_descriptions}

ä½ çš„å·¥ä½œæµç¨‹ï¼š
1. åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œç¡®å®šéœ€è¦å“ªäº›æ•°æ®
2. é€‰æ‹©åˆé€‚çš„å·¥å…·è·å–æ‰€éœ€æ•°æ®
3. å¦‚æœå·²æœ‰æ•°æ®ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œç»§ç»­è°ƒç”¨å…¶ä»–å¿…è¦çš„å·¥å…·
4. åŸºäºæ‰€æœ‰è·å–çš„æ•°æ®ï¼Œæä¾›å…¨é¢ã€æ·±å…¥çš„åˆ†æ

è°ƒç”¨å·¥å…·çš„æ ¼å¼ï¼š
å½“ä½ éœ€è¦è°ƒç”¨å·¥å…·æ—¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼åŒ…è£¹å†…å®¹ï¼š
<tool_call>
{{
  "name": "å·¥å…·åç§°",
  "parameters": {{
    "å‚æ•°1": "å€¼1",
    "å‚æ•°2": "å€¼2"
  }}
}}
</tool_call>

æ³¨æ„äº‹é¡¹ï¼š
- åªåœ¨éœ€è¦è·å–æ•°æ®æ—¶è°ƒç”¨å·¥å…·
- ç¡®ä¿æä¾›å·¥å…·æ‰€éœ€çš„æ‰€æœ‰å¿…è¦å‚æ•°
- è°ƒç”¨å·¥å…·åï¼Œæ ¹æ®è¿”å›ç»“æœå†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è°ƒç”¨å…¶ä»–å·¥å…·
- æœ€ç»ˆåˆ†æåº”åŸºäºæ‰€æœ‰è·å–çš„æ•°æ®ï¼Œç”¨è‡ªç„¶è¯­è¨€æ¸…æ™°è¡¨è¾¾
- åˆ†æåº”åŒ…æ‹¬æŠ€æœ¯é¢åˆ†æã€åŸºæœ¬é¢åˆ†æå’Œæ–°é—»æƒ…æ„Ÿåˆ†æï¼ˆå¦‚é€‚ç”¨ï¼‰
- æ˜ç¡®è¯´æ˜åˆ†æçš„å±€é™æ€§å’Œæ½œåœ¨é£é™©
"""
    
    def _parse_tool_call(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æå¤§æ¨¡å‹çš„å“åº”ï¼Œæå–å·¥å…·è°ƒç”¨æŒ‡ä»¤"""
        start_tag = "<tool_call>"
        end_tag = "</tool_call>"
        
        start_idx = response.find(start_tag)
        end_idx = response.find(end_tag)
        
        if start_idx == -1 or end_idx == -1:
            logger.debug("æœªåœ¨LLMå“åº”ä¸­æ‰¾åˆ°å·¥å…·è°ƒç”¨æ ‡è®°")
            return None
            
        try:
            tool_call_str = response[start_idx + len(start_tag):end_idx].strip()
            logger.debug(f"æå–åˆ°å·¥å…·è°ƒç”¨æ–‡æœ¬: {tool_call_str[:100]}...")
            result = json.loads(tool_call_str)
            logger.debug(f"æˆåŠŸè§£æå·¥å…·è°ƒç”¨JSON: {result['name'] if 'name' in result else 'æœªçŸ¥å·¥å…·'}")
            return result
        except json.JSONDecodeError:
            logger.error("è§£æå·¥å…·è°ƒç”¨JSONå¤±è´¥")
            return None
    
    def _run_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        tool_name = tool_call.get("name")
        parameters = tool_call.get("parameters", {})
        
        logger.info(f"å‡†å¤‡æ‰§è¡Œå·¥å…·: {tool_name}")
        logger.debug(f"å·¥å…·å‚æ•°: {json.dumps(parameters)}")
        
        if not tool_name:
            logger.warning("æœªæŒ‡å®šå·¥å…·åç§°")
            return {"error": "æœªæŒ‡å®šå·¥å…·åç§°"}
        
        tool = self.tool_manager.get_tool(tool_name)
        if not tool:
            logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
            return {"error": f"æœªçŸ¥å·¥å…·: {tool_name}"}
        
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œå·¥å…·: {tool_name}")
            result = tool.run(** parameters)
            logger.info(f"å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
            return {
                "status": "success",
                "tool": tool_name,
                "parameters": parameters,
                "result": result
            }
        except Exception as e:
            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "tool": tool_name,
                "parameters": parameters,
                "error": str(e)
            }
    
    def analyze(self, user_query: str, max_steps: int = 5, step_callback=None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿›è¡Œåˆ†æ"""
        logger.info(f"å¼€å§‹åˆ†æç”¨æˆ·æŸ¥è¯¢: {user_query}")
        self.conversation_history.append({"role": "user", "content": user_query})
        
        steps = []
        final_analysis = None
        
        for step in range(max_steps):
            logger.info(f"æ‰§è¡Œåˆ†ææ­¥éª¤ {step+1}/{max_steps}")
            # è°ƒç”¨å¤§æ¨¡å‹è·å–å“åº”
            logger.debug(f"å‘OpenAIå‘é€è¯·æ±‚ï¼Œæ¨¡å‹: {self.model_name}")
            response = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=self.conversation_history
            )
            
            llm_response = response.choices[0].message.content
            logger.debug(f"æ”¶åˆ°OpenAIå“åº”: {llm_response}")
            self.conversation_history.append({"role": "assistant", "content": llm_response})
            
            # è®°å½•æ­¥éª¤
            step_data = {
                "step": step + 1,
                "llm_response": llm_response
            }
            steps.append(step_data)
            
            # å®æ—¶å‘é€æ€è€ƒè¿‡ç¨‹
            if step_callback:
                # æå–ä¸åŒ…å«å·¥å…·è°ƒç”¨æ ‡è®°çš„éƒ¨åˆ†
                clean_response = llm_response
                tool_call_start = clean_response.find('<tool_call>')
                if tool_call_start != -1:
                    clean_response = clean_response[:tool_call_start].strip()
                
                if clean_response.strip():
                    step_callback({
                        "type": "thinking",
                        "content": f"ğŸ’­ æ­¥éª¤ {step+1}: {clean_response}"
                    })
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            tool_call = self._parse_tool_call(llm_response)
            
            if not tool_call:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜åˆ†æå®Œæˆ
                logger.info("æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œåˆ†æå®Œæˆ")
                final_analysis = llm_response
                break
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            logger.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_call['name'] if tool_call else 'None'}")
            
            # å®æ—¶å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
            if step_callback and tool_call:
                tool_name = tool_call.get("name", "æœªçŸ¥å·¥å…·")
                tool_params = tool_call.get("parameters", {})
                
                # æ ¹æ®å·¥å…·ç±»å‹ç”Ÿæˆæè¿°
                if tool_name == "get_historical_data":
                    ticker = tool_params.get("ticker", "")
                    step_callback({
                        "type": "tool",
                        "content": f"ğŸ“Š æ­£åœ¨è·å– {ticker} çš„å†å²æ•°æ®..."
                    })
                elif tool_name == "get_financial_statements":
                    ticker = tool_params.get("ticker", "")
                    step_callback({
                        "type": "tool",
                        "content": f"ğŸ“ˆ æ­£åœ¨è·å– {ticker} çš„è´¢åŠ¡æŠ¥è¡¨..."
                    })
                elif tool_name == "get_stock_news":
                    ticker = tool_params.get("ticker", "")
                    step_callback({
                        "type": "tool",
                        "content": f"ğŸ“° æ­£åœ¨è·å– {ticker} çš„æœ€æ–°æ–°é—»..."
                    })
                elif tool_name == "calculate_technical_indicators":
                    ticker = tool_params.get("ticker", "")
                    step_callback({
                        "type": "tool",
                        "content": f"ğŸ“‰ æ­£åœ¨è®¡ç®— {ticker} çš„æŠ€æœ¯æŒ‡æ ‡..."
                    })
                else:
                    step_callback({
                        "type": "tool",
                        "content": f"ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name}"
                    })
            
            tool_result = self._run_tool(tool_call)
            steps[-1]["tool_call"] = tool_call
            steps[-1]["tool_result"] = tool_result
            
            # å‘é€å·¥å…·æ‰§è¡Œå®Œæˆä¿¡æ¯
            if step_callback:
                step_callback({
                    "type": "thinking",
                    "content": "âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨åˆ†æç»“æœ..."
                })
            
            # å°†å·¥å…·ç»“æœè¿”å›ç»™å¤§æ¨¡å‹
            # å¤„ç†å¯èƒ½åŒ…å«Timestampç±»å‹é”®çš„å­—å…¸
            def json_serializable(obj):
                import pandas as pd
                if isinstance(obj, dict):
                    return {str(k): json_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [json_serializable(i) for i in obj]
                elif isinstance(obj, pd.Timestamp):
                    return str(obj)
                else:
                    return obj
            
            serializable_result = json_serializable(tool_result)
            tool_result_msg = f"å·¥å…·è°ƒç”¨ç»“æœ:\n{json.dumps(serializable_result, indent=2)}"
            logger.debug(f"å·¥å…·è°ƒç”¨ç»“æœ: {json.dumps(serializable_result)}")
            self.conversation_history.append({"role": "user", "content": tool_result_msg})
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°ä½†è¿˜æ²¡æœ‰æœ€ç»ˆåˆ†æï¼Œè¯·æ±‚ä¸€ä¸ªæ€»ç»“
        if final_analysis is None:
            logger.info("è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œè¯·æ±‚æœ€ç»ˆåˆ†ææ€»ç»“")
            summary_prompt = "è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªå®Œæ•´çš„åˆ†ææ€»ç»“å’ŒæŠ•èµ„å»ºè®®ã€‚ä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ã€‚"
            self.conversation_history.append({"role": "user", "content": summary_prompt})
            
            response = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=self.conversation_history
            )
            
            final_analysis = response.choices[0].message.content
            logger.info("ç”Ÿæˆæœ€ç»ˆåˆ†ææ€»ç»“å®Œæˆ")
            
            # è®°å½•æœ€ç»ˆæ€»ç»“æ­¥éª¤
            steps.append({
                "step": len(steps) + 1,
                "llm_response": final_analysis,
                "is_final_summary": True
            })
        
        # å‘é€æœ€ç»ˆåˆ†æç»“æœ
        if step_callback and final_analysis:
            step_callback({
                "type": "final",
                "content": final_analysis
            })
        
        return {
            "query": user_query,
            "steps": steps,
            "final_analysis": final_analysis,
            "completed": final_analysis is not None
        }
